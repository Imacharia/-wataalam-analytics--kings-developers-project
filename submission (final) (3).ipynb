{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2 PROJECT\n",
    "**Presented by:** Group 4.4\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dennis Mwanzia \n",
    "## Pamela Awino \n",
    "## Ian Macharia\n",
    "## Samuel Igecha\n",
    "## Pauline Njeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0  Project Introduction and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to put into practice our newly aqcuired skills in data science to formulate and solve a real business problem. In this project, we are given a raw dataset which is the house sales prices in Northwestern County and we are supposed to formulate a real business case study and use the data to solve the business problem and provide a useful business advisory which can be used to formulate real and valuable business decisions. Unlike in phase 1, we are not given the business problem, its upon us to formulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_partial_residuals_all' from 'my_functions' (C:\\Users\\USER PC\\Documents\\Flatiron\\WataalamAnalytics\\wataalam-analytics--kings-developers-project\\my_functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7116\\2659283268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmy_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround_bathrooms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_outliers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_coord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_simple_linear_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_multiple_linear_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculate_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_polynomial_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_partial_residuals_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_partial_residuals_all' from 'my_functions' (C:\\Users\\USER PC\\Documents\\Flatiron\\WataalamAnalytics\\wataalam-analytics--kings-developers-project\\my_functions.py)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import folium \n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "from my_functions import clean_data, round_bathrooms, remove_outliers, x_coord, fit_simple_linear_reg, fit_multiple_linear_reg, calculate_rmse, fit_polynomial_reg, plot_partial_residuals_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been provided with King County House Sales dataset found in 'kc_house_data.csv'. There is a description of columns dataset found in the md folder 'column_names.md'. Therefore, we examine the column descriptions to understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpacking the description \n",
    "with open('data/column_names.md', 'r') as f:\n",
    "    md_text = f.read()\n",
    "\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we have been given a dataset that contains the following variables;\n",
    "1. id - unique identified for a house\n",
    "2. dateDate - house was sold\n",
    "3. pricePrice - is prediction target\n",
    "4. bedroomsNumber - of Bedrooms/House\n",
    "5. bathroomsNumber - of bathrooms/bedrooms\n",
    "6. sqft_livingsquare - footage of the home\n",
    "7. sqft_lotsquare - footage of the lot\n",
    "8. floorsTotal - floors (levels) in house\n",
    "9. waterfront - House which has a view to a waterfront\n",
    "10. view - Has been viewed\n",
    "11. condition - How good the condition is ( Overall )\n",
    "12. grade - overall grade given to the housing unit, based on King County grading system\n",
    "13. sqft_above - square footage of house apart from basement\n",
    "14. sqft_basement - square footage of the basement\n",
    "15. yr_built - Built Year\n",
    "16. yr_renovated - Year when house was renovated\n",
    "17. zipcode - zip\n",
    "18. lat - Latitude coordinate\n",
    "19. long - Longitude coordinate\n",
    "20. sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "21. sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business problem is to provide guidance to Kings Wajenzi Developers, a prospective real estate developer in King County, on the most profitable types of properties to target in the current market. The guidance will include recommendations on optimal locations to develop properties, features most desired by customers and have the greatest impact on prices of properties, and season of year when homeowners are most likely to purchase properties. Our objective as Wataalamu Analytics Advisors is to analyze house sales data in King County which was collected between May 2014 and May 2015 and leverage the insights generated to guide the developer in selecting the most profitable properties to develop and maximize their profits. With King County's population on the rise and income stabilizing after the COVID-19 pandemic, demand for new homes in the area is at an all-time high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To determine whether the time of year affects the price of a house and identify the most profitable seasons/months.\n",
    "\n",
    "* To investigate whether location affects house prices in King County and identify the areas that attract the highest prices.\n",
    "\n",
    "* To determine which features/attributes have the highest impact on the sales price of houses/properties in King County and identify the features that developers should focus on when developing upcoming projects in the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guiding Questions\n",
    "####### What to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does the time of the year affect the price of an house? If yes, which seasons/Months are most profitable?\n",
    "2. Does Location affect prices? If yes, which locations within King county attract highest price of houses?\n",
    "3. Which features/Attributes have the highest impact on sales price of houses/properties? Which features should the developer focus on when developing their upcoming projects in King County?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This project follows the OCEMiN Data Science framework for data analysis\n",
    "1.\tObtain/import data\n",
    "* We will import data from King County House Data provided as csv, using Pandas library.\n",
    "2.\tClean/ Scrub data\n",
    "* We will employ several data cleaning methods to modify and prepare the dataset for analysis.\n",
    "3.\tExplore data/EDA\n",
    "* Analyzing and visualizing the cleaned data to gain insights, identify patterns and relationships, and formulate hypotheses.\n",
    "4.\tModel/Develop the predictive Model\n",
    "*  Develop and evaluate predictive models using the data.\n",
    "5.\tInterpret Data\n",
    "* Draw conclusions and make decisions based on the results of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/kc_house_data.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 21597 rows and 21 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset is stored as pandas dataframe. We have three types of datatypes for columns which are integers, floats and objects. Since we are interested in running a linear regression model, we would like all the columns, to be numeric datatypes. However, some columns like `date`, `waterfront`, `view`, `condition`, `grade`, `sqft_basement` are objects and most likely categorical variables and will need to be transformed to dummy variables using one-hot encoding function of python. Intergers will also need to be examined to determine if they are binary variables or numeric data types. \n",
    "Its very important to note that sqft_basement should be of value type float based on the data preview done above. Therefore, we will need to investigate that column to find out why it appears as object instead of float.\n",
    "The `date` column will need to be transformed to month and year columns as we seek to engineer a new feature named `seasons` to answer our first question.\n",
    "The columns `waterfront`, `view`, `condition`, `grade` need to be investigated and necessary transformations done in order to make them dummy variables for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by checking the data for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking for how many missing values\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can see that Waterfront, view and year renovated have missing values. Lets check the percentage of missing values in the dataset. We can then calculate the percentage of missing values to see their magnitude and determine if we will drop the columns or just replace the missing values with appropriate replace values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of missing values\n",
    "for col in ['waterfront', 'view', 'yr_renovated']:\n",
    "    percent_missing = df1[col].isnull().sum() * 100 / len(df1)\n",
    "    print(f\"{col}:{round(percent_missing,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Waterfront`, `view` and `yr_renovated` columns have 11.0%, 0.29%, and 17.79% respectively of missing data. `Waterfront`, and `view` are of data type objects and therefore, we should replace the missing values with the mode. The `yr_renovated` shows the year in which renovations occurred and therefore, we should interpret the missing values to mean that no renovation occurred for those particular properties. Therefore, we will replace the missing values with zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the most common values for `waterfront` and `view` columns which we will use to replace the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Waterfront Mode:\",df1['waterfront'].mode()[0])\n",
    "print (\"view mode:\", df1['view'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us investigate the `sqft_basement` column to find out why it appeard as object data type whereas the vaalues inside show it should be Float data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate sqft_basement column\n",
    "df1['sqft_basement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get summary statistics for sqft_basement\n",
    "df1['sqft_basement'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant number of entries with placeholder `?` as value for `sqft_basement`. Moreover, more than half of entries have a value of 0 indicating the properties did not have a basement and therefore, replace the placeholder with `0`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered the data cleaning was occupying too much of our time and therefore created a function *clean_data* that we applied below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = clean_data(df1).copy()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if all missing values have been resolved\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any missing values, therefore we proceed to check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicates\n",
    "\n",
    "print(\"Number of duplicated values:\", df1.duplicated().sum())\n",
    "\n",
    "duplicate_rows = df1[df1.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two duplicates but upon closer examination, the two rows are not duplicates but rather may have arised as a result of data updates, data merging or entry errors.\n",
    "Next, we can proceed to investigate the integer and object data types and perform the necessary transformations for our multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confirming the data is clean and columns are of proper data types.\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigatin bathroom column\n",
    "# Get unique values\n",
    "df1['bathrooms'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathrooms appear as float and we would expect bathrooms to be integers, so upon researching from common real estate jargon in the United states, bathrooms can be described with floats or fractions to indicate not a full bathroom or with an additional feature such as a sink. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rigourous data clean up, we the try and finetune the data for ploting, visualization and subsequent modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables you want to plot\n",
    "cols_to_plot = df1.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create a subplot grid\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(cols_to_plot), figsize=(32, 6))\n",
    "\n",
    "# Create a boxplot for each variable in a separate subplot\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    axes[i].boxplot(df1[col])\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop the irrelant columns for box plot purposes\n",
    "\n",
    "# Select the categorical variables to drop and then generate boxplots\n",
    "cols_to_plot = df1.select_dtypes(include=['int64', 'float64']).columns.drop([])\n",
    "\n",
    "# Create a subplot grid\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(cols_to_plot), figsize=(30, 6))\n",
    "\n",
    "# Create a boxplot for each variable in a separate subplot\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    axes[i].boxplot(df1[col])\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plots for each data\n",
    "# Select the numerical variables you want to plot\n",
    "num_cols_to_plot = df1.select_dtypes(include=['int64', 'float64']).columns.drop([])\n",
    "print(\"Columns to plot:\",num_cols_to_plot )\n",
    "# Create a histogram for each variable\n",
    "df1[num_cols_to_plot].hist(figsize=(25, 12))\n",
    "plt.savefig('Columnstoplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dropped `view`, `grade`,`condition`, `zipcode` because they are categorical variables and therefore cannot be reasonably visualized using either box plots or histograms.\n",
    "`Floors`, `age`, `year_Sold`, `month_sold` do not appear to have outliers. \n",
    "Therefore, we consider potential outliers in `bedrooms`, `bathrooms`, `sqft_living`, `sqft_lot`, `floors` and `sqft_above` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Investigate bedrooms\n",
    "df1['bedrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The houses with 33, 11 and 10 bedrooms need to be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedroom_counts = [33, 11, 10]\n",
    "columns_to_show = ['floors', 'price', 'sqft_living', 'waterfront', 'bathrooms', 'bedrooms']\n",
    "houses_to_examine = df1[df1['bedrooms'].isin(bedroom_counts)][columns_to_show]\n",
    "print(houses_to_examine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, houses with 10 and 11 bedrooms appear to be appropriate based on their `prices`, `bathrooms`, and `sqft_living`. However, the 33 bedroom house has 2 bathrooms and a sale price of $640000 and it must have been a 3 bedroom house recorded as 33.\n",
    "Therefore, we will replace 33 with 3 bedrooms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accesing the row index for that row using .loc, \n",
    "#we then specify the column and replace as described above.\n",
    "df1.loc[15856, 'bedrooms'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove other outliers,  we will apply the statistical method, that calculate the z-score of the feature columns. \n",
    "The functiion takes in the dataframe , calculates the z-score per column then removes any value that falls within $z-score > |3|$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to df\n",
    "kc_house_data_clean = remove_outliers(df1)\n",
    "\n",
    "# saving the clean data to a new dataframe\n",
    "kc_house_data_clean.to_csv('data/kc_house_data_clean.csv', index=False)\n",
    "\n",
    "# Reading the now cleaned data again\n",
    "df = pd.read_csv('data/kc_house_data_clean.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Data Exploration\n",
    "   > for the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Investigating Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to investigate `price` and its relationship with house features, however since price appears to have outliers, i.e. houses with very high prices and others with very low prices. We will group price into three categories; Upper price, Median price and Low price where, upper price = 75% percentile, median price = 50% percentile and the low price = 25% percentile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to Group price into three cartegories \n",
    "# Define the percentile values for each category\n",
    "price = df['price']\n",
    "high_percentile = np.percentile(price, 75)\n",
    "low_percentile = np.percentile(price, 25)\n",
    "\n",
    "# Group the prices into categories based on the percentiles\n",
    "high_prices = price[price > high_percentile]\n",
    "medium_prices = price[(price >= low_percentile) & (price <= high_percentile)]\n",
    "low_prices = price[price < low_percentile]\n",
    "\n",
    "# Print the results\n",
    "#print(\"High prices:\", high_prices)\n",
    "#print(\"Medium prices:\", medium_prices)\n",
    "#print(\"Low prices:\", low_prices)\n",
    "\n",
    "print(\"Highest Price:\", high_prices.max())\n",
    "print(\"Median Price:\", medium_prices.median())\n",
    "print(\"Lowest price:\", low_prices.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do different priced houses relate with the number  of bedrooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['bedrooms']\n",
    "y1 = high_prices\n",
    "y2 = medium_prices\n",
    "y3 = low_prices\n",
    "\n",
    "# set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# set colors\n",
    "colors = [\"#FFC300\", \"#DAF7A6\", \"#FF5733\"]\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(14,6), sharey=True)\n",
    "\n",
    "# bar plot\n",
    "sns.barplot(x=x, y=y1, color=colors[2], ax=ax[0])\n",
    "ax[0].set_title(\"High prices\", fontsize=14)\n",
    "ax[0].set_xlabel(\"Bedrooms\", fontsize=12)\n",
    "ax[0].set_ylabel(\"Price ($)\", fontsize=12)\n",
    "\n",
    "# bar plot\n",
    "sns.barplot(x=x, y=y2, color=colors[1], ax=ax[1])\n",
    "ax[1].set_title(\"Median prices\", fontsize=14)\n",
    "ax[1].set_xlabel(\"Bedrooms\", fontsize=12)\n",
    "ax[1].set_ylabel(\"Price ($)\", fontsize=12)\n",
    "\n",
    "# bar plot\n",
    "sns.barplot(x=x, y=y3, color=colors[0], ax=ax[2])\n",
    "ax[2].set_title(\"Low prices\", fontsize=14)\n",
    "ax[2].set_xlabel(\"Bedrooms\", fontsize=12)\n",
    "ax[2].set_ylabel(\"Price ($)\", fontsize=12)\n",
    "\n",
    "# set title for the whole figure\n",
    "fig.suptitle(\"Price vs Bedrooms\", fontsize=16)\n",
    "\n",
    "# adjust spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "# show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the larger the number of bedrooms, the higher the prices as well. For average and low-priced houses, there isn't much of an increase but more of constant with a slight increase in prices with the number of bedrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How do different priced houses relate with the number  of bathrooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting price against bathrooms\n",
    "#data\n",
    "x = df['bathrooms']\n",
    "y1 = high_prices\n",
    "y2 = medium_prices\n",
    "y3 = low_prices\n",
    "\n",
    "# Histogram plots\n",
    "# High prices vs No. of bathrooms\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, ncols=1, figsize=(10, 20))\n",
    "\n",
    "sns.histplot(x=x, y=y1, ax=ax0, color ='green')\n",
    "plt.title('High-end properties Vs No. of Bathrooms', fontsize=18)\n",
    "plt.xlabel('No of bathrooms', fontsize=14)\n",
    "plt.ylabel('High Prices', fontsize=14);\n",
    "\n",
    "# median price vs No. of bathrooms\n",
    "#fig, ax1 = plt.subplots( figsize=(10,8))\n",
    "sns.histplot(x=x, y=y2, ax=ax1, color ='pink')\n",
    "plt.title('Middle Price Properties Vs No. of Bathrooms', fontsize=18)\n",
    "plt.xlabel('No of bathrooms', fontsize=14)\n",
    "plt.ylabel('Middle Prices', fontsize=14);\n",
    "\n",
    "# low price vs No. of bathrooms\n",
    "#fig, ax2 = plt.subplots( figsize=(10,8), facecolor ='red')\n",
    "sns.histplot(x=x, y=y3, ax=ax2, color ='blue')\n",
    "plt.title('Low-price Properties Vs No. of Bathrooms', fontsize=18)\n",
    "plt.xlabel('No of bathrooms', fontsize=14)\n",
    "plt.ylabel('Low Prices', fontsize=14);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dataentries per bedrooms\n",
    "df['bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Histogram we observed that the majority of median and high-end properties had a higher density at two bathrooms with and aditional bathtub, toilet or sink. \n",
    "From the high end we also noticed that the density was more towards the median showing that there is little difference that distinguished them from the median. \n",
    "For the low price range the density was more on one bedroomed houses indicating a prevalence of studio apartments.\n",
    "\n",
    "The histograms suggest that the number of bathrooms is positively correlated with the price of properties. However, we need to perform statistical analysis to confirm this relationship and identify the strength of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How do different-priced houses relate with the seasons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary mapping month numbers to month names\n",
    "import calendar\n",
    "\n",
    "month_dict = dict(enumerate(calendar.month_name))\n",
    "\n",
    "# apply the dictionary to the month_sold column to get month names\n",
    "df['month_name'] = df['month_sold'].apply(lambda x: month_dict[x])\n",
    "\n",
    "# define seasons based on months\n",
    "seasons = {'Winter': ['December', 'January', 'February'],\n",
    "           'Spring': ['March', 'April', 'May'],\n",
    "           'Summer': ['June', 'July', 'August'],\n",
    "           'Fall': ['September', 'October', 'November']}\n",
    "\n",
    "# create a new 'season' column based on the 'month_sold' column\n",
    "df['season'] = df['month_name'].apply(lambda x: next((season for season, months in seasons.items() if x in months), None))\n",
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting price vs seasons\n",
    "\n",
    "x = df['season']\n",
    "y1 = high_prices\n",
    "y2 = medium_prices\n",
    "y3 = low_prices\n",
    "\n",
    "\n",
    "# Scatter plots\n",
    "\n",
    "# High prices vs No. of bathrooms\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, ncols=1, figsize=(10, 20))\n",
    "sns.scatterplot(x=x, y=y1, ax=ax0, color='blue')\n",
    "plt.title('High-end properties Vs Seasons', fontsize=18)\n",
    "plt.xlabel('Season', fontsize=14)\n",
    "plt.ylabel('High Prices', fontsize=14);\n",
    "\n",
    "# median price vs No. of bathrooms\n",
    "#fig, ax1 = plt.subplots( figsize=(10,8))\n",
    "sns.scatterplot(x=x, y=y2, ax=ax1, color='red')\n",
    "plt.title('Middle Price Properties Vs Seasons', fontsize=18)\n",
    "plt.xlabel('Season', fontsize=14)\n",
    "plt.ylabel('Middle Prices', fontsize=14);\n",
    "\n",
    "# low price vs No. of bathrooms\n",
    "#fig, ax2 = plt.subplots( figsize=(10,8))\n",
    "sns.scatterplot(x=x, y=y3, ax=ax2, color='k')\n",
    "plt.title('Low-price Properties Vs Seasons', fontsize=18)\n",
    "plt.xlabel('Season', fontsize=14)\n",
    "plt.ylabel('Low Prices', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided data, the scatter plots show the relationship between the season and the prices of properties in the dataset. From the count of properties sold in each season, it appears that spring and summer are the most popular seasons for home buying, while fall and winter have comparatively fewer sales.\n",
    "\n",
    "The scatter plots of high-end, medium, and low-price properties against the season show that the prices of properties are generally consistent across all seasons. However, there is a slight increase in the number of high-end properties sold in the spring and a decrease in the number of low-priced properties sold in the winter.\n",
    "\n",
    "This pattern may be due to various factors, including the perceived desirability of certain seasons for buying and selling homes, the availability of listings during different seasons, or even the preferences of buyers and sellers. For example, the spring season may be associated with better weather conditions and more opportunities for home viewing, which may increase demand for high-end properties. Conversely, the winter season may be associated with lower sales of low-priced properties due to holiday expenses and colder weather. However, further analysis would be necessary to determine the exact reasons for these trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by season and month_sold, and count the number of sales for each group\n",
    "season_sales = df.groupby(['season', 'month_sold'])['price'].count()\n",
    "# Find the season with the most sales\n",
    "\n",
    "best_season = season_sales.groupby('season').max()\n",
    "\n",
    "# Sort the seasons by total sales in descending order\n",
    "best_season = best_season.sort_values(ascending=False)\n",
    "\n",
    "# Print the season and corresponding months with the amount sold in each season\n",
    "for season in best_season.index:\n",
    "    months = ', '.join(seasons[season])\n",
    "    sales = season_sales.loc[season]\n",
    "    print(f\"{season} ({months}): {sales.sum()} sales\")\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to show the sales for each season\n",
    "sns.barplot(x=best_season.index, y=best_season.values)\n",
    "plt.title('Season-wise sales')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sales data, we can see that the spring months of March, April, and May are the best for selling houses with a total of 6518 sales. This is likely due to the fact that Spring is a time of renewal and growth, and people may be more willing to make big changes such as buying a new home during this time. May is the most popular month for selling homes within Spring, with 2414 sales.\n",
    "\n",
    "Summer, which comprises the months of June, July, and August, had a slightly lower total sales of 6328 compared to spring. July had the highest sales with 2211 followed by June with 2178 and \n",
    "August with 1939.\n",
    "\n",
    "The fall season of September, October, and November had a total of 5056 sales, with October having the highest sales of 1876 followed by September with 1771 and November with 1409.\n",
    "\n",
    "Lastly, the winter season of December, January, and February had the lowest total sales of 3695. December had the highest sales with 1470 followed by February with 1247 and January with 978. This may be due to the fact that Winter is a time when people tend to stay indoors and may be less inclined to go through the hassle of buying or selling a home.\n",
    "\n",
    "Overall, it is important to note that this conclusion is based solely on the provided data and may not be reflective of larger trends in the housing market. Additionally, there may be some inaccuracies in the data such as missing or incomplete records, which could affect the accuracy of these findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. How does Location affect the Sale Price of a house?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location is key when it comes to real estate. Our second objective seeks to understand the geographical distribution of the homes in our dataset and determine where the highest house sales were recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to manipulate\n",
    "dfq2 = df.copy()\n",
    "\n",
    "# Plot scatter plot\n",
    "plt.figure(figsize = (11,9))\n",
    "ax = sns.scatterplot(x = dfq2['long'], y = dfq2['lat'], hue = dfq2['price'],\n",
    "                palette = 'nipy_spectral',s=10, legend = None, data = dfq2)\n",
    "\n",
    "norm = plt.Normalize(dfq2['price'].min(), dfq2['price'].max())\n",
    "smap = plt.cm.ScalarMappable(cmap='nipy_spectral', norm=norm)\n",
    "smap.set_array([])\n",
    "\n",
    "ax.figure.colorbar(smap, label = 'price in USD')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('King County House Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visualisation we can already draw initial insights based on the houses' geographical locations. The highest house prices are concentrated in the area with latitude around 47.6 and longitude around -122.25. There is a disparity with southern locations achieving lower house prices.\n",
    "\n",
    "However there is no context to help us! We would need an underlying map to understand where the cities, highways, bodies of water etc. are located. It is difficult to draw meaningful conclusions with this visualisation alone.\n",
    "\n",
    "Whilst we could define quadrants using the latitude and longitude, let us seek to plot the houses on a map instead.\n",
    "\n",
    "Map of house sales To visualise house sales on a map, we will use Bokeh and the built-in map tile from Carto DB. Note that Bokeh requires Mercator coordinates so we will need to convert our latitude and longitude values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coord as tuple (lat,long)\n",
    "dfq2['coord'] = list(zip(dfq2['lat'], dfq2['long']))\n",
    "\n",
    "# We will create new columns mercator_x and mercator_y to use in our plot.\n",
    "\n",
    "# Obtain list of mercator coordinates using helper function\n",
    "mercators = [x_coord(x, y) for x, y in dfq2['coord'] ]\n",
    "\n",
    "# Create column mercator\n",
    "dfq2['mercator'] = mercators\n",
    "\n",
    "# Define columns mercator_x and mercator_y\n",
    "dfq2[['mercator_x', 'mercator_y']] = dfq2['mercator'].apply(pd.Series)\n",
    "\n",
    "# Preview relevant columns\n",
    "dfq2[['zipcode', 'mercator_x', 'mercator_y', 'price']].head()\n",
    "\n",
    "# Get top 100 prices\n",
    "dftop = dfq2.sort_values('price', ascending = False)[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot with follium\n",
    "\n",
    "# Create a base map by providing coordinates\n",
    "m = folium.Map(location = [47.5, -122.2])\n",
    "\n",
    "# Add data for heatmap\n",
    "heat_data = dftop['coord']\n",
    "\n",
    "# Add heatmap onto map\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pinpoint these directly using the coordinates. To increase readability, we have chosen to only display the top 30 houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 30 houses on map\n",
    "\n",
    "# Create a base map by providing coordinates\n",
    "m = folium.Map(location = [47.5, -122.2])\n",
    "\n",
    "# Add markers for houses\n",
    "for coordinates in dftop['coord'][:30]:\n",
    "    folium.Marker([coordinates[0], coordinates[1]], popup = coordinates).add_to(m)\n",
    "\n",
    "# Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above codes may slow down an old machine therefore we created a seperate plot using a lighter package called plotly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the overlay the above data on a topographical map to view the locations and observe additional features\n",
    "This proved useful as we are able to clearly see the areas where the high end properties were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Define the map layout\n",
    "layout = go.Layout(\n",
    "    title='Housing Sales by Location',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        style='stamen-terrain',\n",
    "        bearing=0,\n",
    "        center=dict(\n",
    "            lat=df['lat'].median(),\n",
    "            lon=df['long'].median()\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=10\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define the map data as a scatter plot of the coordinates\n",
    "data = go.Scattermapbox(\n",
    "    lat=df['lat'],\n",
    "    lon=df['long'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=df['price'],\n",
    "        \n",
    "        opacity=0.8\n",
    "    ),\n",
    "    text=['Price: ${}'.format(i) for i in df['price']],\n",
    "    hovertext = df.apply(lambda x: f\"Price: ${x['price']}, Waterfront: {x['waterfront']}\", axis=1),\n",
    ")\n",
    "\n",
    "# Create the map figure and show it\n",
    "fig = go.Figure(data=[data], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plotted graph, it is evident that a significant number of high-end residences are situated in the Bellevue area as well as along the waterfront. The region comprises of a considerable size of uninhabitable mountainous terrain, which concentrates most of the housing in the towns. As a result, these towns appear to have a higher density of housing compared to the surrounding areas. \n",
    "\n",
    "Additionally, it is important to note that the plot may not be entirely accurate and there may be some data points that are not a representative of the actual situation. For example, some properties may be incorrectly labeled as having waterfront access when they do not, which could affect their price and skew the distribution shown in the plot. \n",
    "\n",
    "However, it is important to note that this statement is limited to the data presented in the plot and may not necessarily represent the overall distribution of residences in the area. Further analysis and data may be required to make more conclusive observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Waterfront"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Does the Presence of a waterfront feature affect its price?\n",
    "\n",
    "From the maps, it is apparent that being on the waterfront is highly sought after and many of the most expensive houses in our dataset have this feature. We have been provided with a waterfront feature, which characterises houses which have a view of a waterfront. Let us investigate how this feature relates to price by using a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot of waterfront feature\n",
    "sns.boxplot(x = dfq2['waterfront'], y = dfq2['price'])\n",
    "plt.title(\"Boxplot of waterfront feature vs. price\")\n",
    "plt.ylabel(\"price in USD\")\n",
    "plt.xlabel(None)\n",
    "plt.xticks(np.arange(2), ('Not view of waterfront', 'Waterfront view'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfrontmean = dfq2[dfq2['waterfront'] == 1]['price'].mean()\n",
    "nonwaterfrontmean = dfq2[dfq2['waterfront'] == 0]['price'].mean()\n",
    "print(f\"The mean house price for a house with waterfront view is USD {round(waterfrontmean,2)}\")\n",
    "print(f\"The mean house price for a house without waterfront view is USD {round(nonwaterfrontmean,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfq2[dfq2['waterfront'] == 1])/len(dfq2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis we can see that most houses in King County have no waterfront.\n",
    "However, the houses with waterfront have some of the highest prices. Confirming our assumtion from the plot before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore zipcode\n",
    "df['zipcode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70 unique zipcodes represented in our dataset.\n",
    "\n",
    "Let us create a scatter plot to explore the median house price per zipcode and see if we can establish zipcode 'tiers'. We will make use of a geojson file showing the zip code boundaries and extract price and zipcode data from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract price and zipcode from our dataset\n",
    "dfrel = df[['zipcode', 'price']]\n",
    "\n",
    "# Groupby zipcode and take median price\n",
    "dfrel = dfrel.groupby(dfrel['zipcode']).median()\n",
    "\n",
    "# Reset index after groupy\n",
    "dfrel = dfrel.reset_index()\n",
    "\n",
    "# Change type to str\n",
    "dfrel['zipcode'] = dfrel['zipcode'].astype(str)\n",
    "\n",
    "# Rename column to match type\n",
    "dfrel = dfrel.rename(columns = {'zipcode' : 'ZIPCODE'})\n",
    "\n",
    "# Check\n",
    "dfrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Zipcode\n",
    "\n",
    "\n",
    "# Count the number of occurrences of each zipcode\n",
    "zipcode_counts = dfrel['ZIPCODE'].value_counts()\n",
    "\n",
    "# Get the 10 most common zipcodes\n",
    "top_zipcodes = zipcode_counts.head(10).index\n",
    "\n",
    "# Filter the data to include only the top zipcodes\n",
    "filtered_df = dfrel[dfrel['ZIPCODE'].isin(top_zipcodes)]\n",
    "\n",
    "# Set the Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Create the scatter plot\n",
    "sns.scatterplot(x=\"ZIPCODE\", y=\"price\", data=filtered_df, s=100, color=\"purple\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"ZIPCODE\", fontsize=14)\n",
    "plt.ylabel(\"Price\", fontsize=14)\n",
    "plt.title(\"House Prices by Zipcode\", fontsize=16)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Modeling\n",
    "\n",
    "The objective of this section is to develop a model that accurately predicts sales price of properties in King County, when given various exogenous variables.\n",
    "The modeling procedure encompasses the selection of suitable algorithms, feature engineering, fine-tuning of hyperparameters, and evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data analysis, it appears that most of the variables have a linear relationship to the price, and their relationship appears to be homoscedastic. However, there are some variables, including sqft_lot, floors, condition, latitude, longitude and sqft_lot15, that do not seem to have a linear relationship with price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'season', 'month_name', 'month_sold', 'year_sold', 'grade', 'condition', 'view', 'waterfront', 'zipcode'], axis=1)\n",
    "fig, axs = plt.subplots(3, 4, figsize=(16, 16), sharey=True)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if i*4 + j < len(X.columns):\n",
    "            sns.regplot(x=X.iloc[:,i*4+j], y='price', data=df, ax=axs[i][j])\n",
    "            axs[i][j].set_title(X.columns[i*4+j])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include='number').columns.drop('price')\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, center=0)\n",
    "plt.savefig('Images/multi-collinearity.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at correlations between other variables and price\n",
    "df.corr()[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Sqft_living is the feature with the strongest correlation in our dataset let's build our baseline model with that\n",
    "df.plot.scatter(x=\"sqft_living\", y=\"price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a fairly clear linear relationship between the Square footage of living space feature and price of a home in King County. We can use this to make a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "y = df[\"price\"]\n",
    "X_baseline = df[[\"sqft_living\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = fit_simple_linear_reg(df, 'price', 'sqft_living')\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display model coefficients\n",
    "baseline_results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model results\n",
    "\n",
    "Looking at our summary above, our regression line is\n",
    " $$ Price = $$33,399 +  239 * sqft\\_living  \n",
    "\n",
    "We can make the following observations from the results:\n",
    "\n",
    "* The model is statistically significant with a p-value below 0.05.\n",
    "* The model explains 40.1% of the variance in price, as indicated by the adjusted R-squared value.\n",
    "* Both the intercept (const) and the slope (sqft_living) coefficients are statistically significant with t-statistic p-  values well below 0.05.\n",
    "* For a house with a living space of 1000 square feet, we would expect the price to be about $272399.\n",
    "\n",
    "* Generally speaking, as the living space of the house increases, so does the price of the house. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Baseline Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"sqft_living\", y=\"price\", label=\"Data points\", ax=ax)\n",
    "sm.graphics.abline_plot(model_results=baseline_results, label=\"Regression line\", ax=ax, color=\"red\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create partial regression plots for the predictor\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "sm.graphics.plot_regress_exog(baseline_results, \"sqft_living\", fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 1\n",
    "predictor_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'condition', 'month_sold','age','sqft_lot','yr_renovated'] \n",
    "multiple_model = fit_multiple_linear_reg(df, 'price', predictor_cols)\n",
    "print(multiple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  R-squared value (0.5312489269063179), indicates that the model explains 53% of the variations in response variable.\n",
    "\n",
    "The test training R-squared value indicates that the model is likely overfitting to the training data and not generalizing well to new data.\n",
    "\n",
    "These results suggest that the multiple linear regression model may have overfit the training data, leading to poor performance on the test set. Further analysis and possibly model tuning may be needed to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictor_cols1 = [\"sqft_living\", \"bedrooms\",\"bathrooms\",\"sqft_lot\",\"floors\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "multiple_model1 = fit_multiple_linear_reg(df, 'price', predictor_cols1)\n",
    "print(multiple_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that the multiple linear regression model may have overfit the training data, leading to poor performance on the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression\n",
    "predictor_cols = ['sqft_living']\n",
    "degree = 2\n",
    "poly_test = fit_polynomial_reg(df, 'price', predictor_cols, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that the polynomial regression model may have overfit the training data, leading to poor performance on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for performing polynomial regression is to capture more complex, nonlinear relationships between the predictor and target variables.\n",
    "\n",
    "In simple linear regression, you model the relationship between the target and a single predictor variable using a straight line. However, in many real-world scenarios, the relationship between the target and the predictor may not be linear, but rather curvilinear or have some other shape.\n",
    "Polynomial regression addresses this issue by adding polynomial features to the model, which allow it to capture more complex relationships between the predictors and the target variable. By doing so, it can improve the model's fit to the data and potentially lead to better predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initial feature colums\n",
    "predictor_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'zipcode']\n",
    "predictor_cols_results = fit_simple_linear_reg(df, 'price', predictor_cols)\n",
    "print(predictor_cols_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our summary above, our regression line is\n",
    " $$ Price = -5.078e+07-4.645e+04 * bedrooms+2.991e+04 bathrooms + 254.5901 sqft_living + 8277.1965 floors + 6.573e+05 waterfront+518.6152 zipcode\n",
    "\n",
    "In this case, the R-squared value is 0.456, which means that the independent variables explain about 45.6% of the variance in house prices.\n",
    "\n",
    "The second section of the output provides the coefficients and standard errors of the independent variables. The coefficient represents the change in the dependent variable that is associated with a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "\n",
    "The constant term is -5.078e+07, which is the estimated value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "For the remaining variables:\n",
    "\n",
    "A one-unit increase in 'bedrooms' is associated with a decrease of $46,450 in house prices, holding all other variables constant.\n",
    "A one-unit increase in 'bathrooms' is associated with an increase of $29,910 in house prices, holding all other variables constant.\n",
    "A one-unit increase in 'sqft_living' is associated with an increase of $254.59 in house prices, holding all other variables constant.\n",
    "A one-unit increase in 'floors' is associated with an increase of $8,277.20 in house prices, holding all other variables constant.\n",
    "A house located in a waterfront area has a mean house price that is $657,300 higher than a house not located in a waterfront area, holding all other variables constant.\n",
    "\n",
    "In this output, all independent variables have p-values less than 0.05, indicating that they are statistically significant predictors of house prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Moving to our second list of features to see if it will improve. \n",
    "predictor_cols1 = [\"sqft_living\", \"bedrooms\",\"bathrooms\",\"sqft_lot\",\"floors\",\"sqft_above\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "predictor_cols1_results = fit_simple_linear_reg(df, 'price', predictor_cols1)\n",
    "print(predictor_cols1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared value of 0.464 indicates that the model explains 46.4% of the variance in the dependent variable. The adjusted R-squared value is the same, which suggests that adding or removing explanatory variables does not significantly improve the fit of the model.\n",
    "\n",
    "The P-values (P>|t|) indicate the statistical significance of each variable. A low P-value (typically below 0.05) suggests that the variable is statistically significant and has a significant effect on the dependent variable.\n",
    "\n",
    "The intercept or constant term is -4.143e+07, which represents the estimated value of the dependent variable when all independent variables are equal to zero.\n",
    "\n",
    "The \"const\" coefficient represents the intercept or constant term, which is the estimated value of the dependent variable when all independent variables are equal to zero. In this case, the intercept is -4.143e+07, which means that if all other independent variables are zero, the estimated value of \"price\" is negative 41,430,000 dollars. However, since it's not possible for all independent variables to be zero in practice, the intercept should be interpreted with caution.\n",
    "\n",
    "The \"sqft_living\" coefficient is 272.8535, which means that a one-unit increase in \"sqft_living\" is associated with an increase in \"price\" of 272.8535 dollars, holding all other variables constant.\n",
    "\n",
    "The \"bedrooms\" coefficient is -4.704e+04, which means that a one-unit increase in \"bedrooms\" is associated with a decrease in \"price\" of 47,040 dollars, holding all other variables constant.\n",
    "\n",
    "The \"bathrooms\" coefficient is 2.414e+04, which means that a one-unit increase in \"bathrooms\" is associated with an increase in \"price\" of 24,140 dollars, holding all other variables constant.\n",
    "\n",
    "The \"sqft_lot\" coefficient is -0.9808, which means that a one-unit increase in \"sqft_lot\" is associated with a decrease in \"price\" of 0.9808 dollars, holding all other variables constant.\n",
    "\n",
    "The \"floors\" coefficient is 1.286e+04, which means that a one-unit increase in \"floors\" is associated with an increase in \"price\" of 12,860 dollars, holding all other variables constant.\n",
    "\n",
    "The \"sqft_above\" coefficient is -19.5153, which means that a one-unit increase in \"sqft_above\" is associated with a decrease in \"price\" of 19.5153 dollars, holding all other variables constant.\n",
    "\n",
    "\n",
    "The \"waterfront\" coefficient is 6.438e+05, which means that a property located on the waterfront is associated with an increase in \"price\" of 643,800 dollars, holding all other variables constant.\n",
    "\n",
    "\n",
    "\n",
    "Overall, the model appears to have a good fit based on the high F-statistic and low Prob (F-statistic) value, indicating that the explanatory variables as a whole are statistically significant in explaining the dependent variable. However, it is important to further evaluate the model's assumptions and check for potential issues such as multicollinearity, heteroscedasticity, or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now our model will match same number of parameters as the first multimodel \n",
    "predictor_cols2 = [\"sqft_living\",\"bathrooms\",\"floors\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "predictor_cols2_results = fit_simple_linear_reg(df, 'price', predictor_cols2)\n",
    "print(predictor_cols2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constant term is -5.076e+07, which represents the estimated value of \"price\" when all the independent variables are zero.\n",
    "\n",
    "For \"sqft_living\", the coefficient is 226.2587, which indicates that, on average, a one-unit increase in square footage of living space is associated with an increase of $226.26 in the price of the house.\n",
    "\n",
    "For \"bathrooms\", the coefficient is 1.918e+04, which indicates that, on average, a one-unit increase in the number of bathrooms is associated with an increase of $19,180 in the price of the house.\n",
    "\n",
    "For \"floors\", the coefficient is 1.462e+04, which indicates that, on average, a one-unit increase in the number of floors is associated with an increase of $14,620 in the price of the house.\n",
    "\n",
    "For \"yr_renovated\", the coefficient is 63.1908, which indicates that, on average, a one-unit increase in the year of renovation is associated with an increase of $63.19 in the price of the house.\n",
    "\n",
    "For \"waterfront\", the coefficient is 6.643e+05, which indicates that, on average, a house with a waterfront location is associated with an increase of $664,300 in the price of the house compared to a house without a waterfront location.\n",
    "\n",
    "In summary, the model suggests that the square footage of living space, number of bathrooms, number of floors, year of renovation, and waterfront location are significant predictors of the house price.\n",
    "\n",
    "Overall, the model explains 45.1% of variations in response variable. It appears a good fit since its statistically significant even though it explains low percentage of variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Residuals for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_residuals_all(results, predictor_cols):\n",
    "    \"\"\"\n",
    "    Plots the residuals for all predictor variables against their observed values.\n",
    "\n",
    "    Parameters:\n",
    "    results (OLSRegressionResults): the results of a fitted linear regression model\n",
    "    predictor_cols (list of str): the names of the predictor columns\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(len(predictor_cols), 1, figsize=(8, 3*len(predictor_cols)), sharex=False)\n",
    "\n",
    "    for i, col in enumerate(predictor_cols):\n",
    "        axs[i].scatter(results.model.exog[:, i+1], results.resid, alpha=0.5)\n",
    "        axs[i].set_xlabel(col)\n",
    "        axs[i].set_ylabel('Residual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'zipcode']\n",
    "plot_residuals_all(predictor_cols_results, predictor_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictor_cols1 = [\"sqft_living\", \"bedrooms\",\"bathrooms\",\"sqft_lot\",\"floors\",\"sqft_above\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "plot_residuals_all(predictor_cols1_results, predictor_cols1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cols2 = [\"sqft_living\",\"bathrooms\",\"floors\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "\n",
    "plot_residuals_all(predictor_cols2_results, predictor_cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Fitted Values for Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_fitted_values_all(results, predictor_cols):\n",
    "    \"\"\"\n",
    "    Plots the fitted values for all predictor variables against their observed values.\n",
    "\n",
    "    Parameters:\n",
    "    results (OLSRegressionResults): the results of a fitted linear regression model\n",
    "    predictor_cols (list of str): the names of the predictor columns\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(len(predictor_cols), 1, figsize=(8, 3*len(predictor_cols)), sharex=False)\n",
    "\n",
    "    for i, col in enumerate(predictor_cols):\n",
    "        axs[i].scatter(results.model.exog[:, i+1], results.fittedvalues, alpha=0.5)\n",
    "        axs[i].set_xlabel(col)\n",
    "        axs[i].set_ylabel('Fitted Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'zipcode']\n",
    "predictor_cols_results\n",
    "plot_fitted_values_all(predictor_cols_results, predictor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictor_cols1 = [\"sqft_living\", \"bedrooms\",\"bathrooms\",\"sqft_lot\",\"floors\",\"sqft_above\",\"yr_renovated\", \"waterfront\", \"zipcode\"]\n",
    "predictor_cols1_results\n",
    "plot_fitted_values_all(predictor_cols1_results, predictor_cols1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the fit\n",
    "plot_fit = predictor_cols2_results.fittedvalues\n",
    "\n",
    "# Plotting the fitted values\n",
    "fig, axs = plt.subplots(ncols=len(predictor_cols2), figsize=(20, 4))\n",
    "\n",
    "for i, col in enumerate(predictor_cols2):\n",
    "    axs[i].scatter(df[col], plot_fit, alpha=0.5)\n",
    "    axs[i].set_xlabel(col)\n",
    "    axs[i].set_ylabel('Fitted Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Model 3 appears to be a better model for predicting house prices in King County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rmse ## metrics?\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Fit a linear regression model on the training data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "# Predict the target variable on the testing data\n",
    "y_pred = lr.predict(X_test)\n",
    "# Calculate the RMSE\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE value of 186162.44586536638 indicates that the root mean square difference between the predicted and actual values is approximately 186162.45 units. The units will depend on the scale of the target variable, but it is typically the same as the units of the target variable.\n",
    "\n",
    "A lower RMSE value indicates better performance, as it means the model has a smaller difference between the predicted and actual values. Conversely, a higher RMSE value indicates worse performance, as it means the model has a larger difference between the predicted and actual values.\n",
    "\n",
    "It's important to note that the interpretation of RMSE values depends on the context of the problem and the scale of the target variable. It's often useful to compare the RMSE of different models or the same model with different sets of parameters to determine which one performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model applying One Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = df\n",
    "\n",
    "# Separate the target variable\n",
    "y = df['price']\n",
    "X = df.drop('price', axis=1)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "categorical_cols = ['waterfront', 'view', 'condition', 'grade']\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "X_encoded = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "\n",
    "# Scale numeric variables\n",
    "numeric_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement', 'age', 'renovated']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Combine one-hot encoded and scaled numeric variables\n",
    "X_processed = pd.concat([X_encoded, pd.DataFrame(X_scaled, columns=numeric_cols)], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Training score:', model.score(X_train, y_train))\n",
    "print('Testing score:', model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the model having added cartegorical variables by method of OHE(one hot encoding) which applies to the categorical variables and standard scaling to the numerical variables. replacing each cartegory with 1 where is appears and the rest 0. creating separate columns of that variable.\n",
    "\n",
    "The training score and testing score are measures of how well the linear regression model is able to predict the target variable (price) based on the input features (bedrooms, bathrooms, etc.).\n",
    "\n",
    "The training score is the R-squared value of the model on the training set. R-squared is a statistical measure that represents the proportion of the variance in the target variable that is explained by the input features. In this case, the training score of 0.529 indicates that the input features explain 52.9% of the variance in the target variable in the training set.\n",
    "\n",
    "The testing score is the R-squared value of the model on the testing set. This score is a measure of how well the model generalizes to new, unseen data. In this case, the testing score of 0.537 indicates that the input features explain 53.7% of the variance in the target variable in the testing set.\n",
    "\n",
    "Since the training and testing scores are relatively close to each other, it suggests that the model is not overfitting (performing well on the training data but poorly on the testing data). However, the overall score is relatively low, which indicates that the input features do not explain a large proportion of the variance in the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "(Principal Component Analysis) is a widely used statistical technique for dimensionality reduction of high-dimensional data. It aims to transform the original data into a new, lower-dimensional feature space while preserving as much of the original variation or structure in the data as possible.\n",
    "\n",
    "PCA is used for dimensionality reduction of numerical data by transforming the data into linearly uncorrelated principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the data\n",
    "df = df\n",
    "\n",
    "# Separate the target variable\n",
    "# = df['price']\n",
    "# = df.drop(index=row_index)\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "#caler = StandardScaler()\n",
    "#_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Use the transformed features X_pca to fit your model.\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2)\n",
    "\n",
    "# Train the linear regression model\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model with desired $R^2$ value\n",
    "#### Improved OHE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Select all numeric columns except price\n",
    "    numeric_cols = [col for col in df.columns if col != 'price' and np.issubdtype(df[col].dtype, np.number)]\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "    \n",
    "    # Combine scaled data and categorical data\n",
    "    X = np.concatenate([X_scaled, df_encoded.drop(numeric_cols, axis=1).values], axis=1)\n",
    "    \n",
    "    # Get target variable\n",
    "    y = df['price'].values\n",
    "    \n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(df)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first array(X) is a numeric array of standardized data where each row represents a house in a dataset. The first column represents the standardized values of the first numeric variable, the second column represents the standardized values of the second numeric variable, and so on. The last column, which is not standardized, represents the price of each house.\n",
    "\n",
    "The second array(y) represents the price of each house in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit OLS model\n",
    "X, y = preprocess_data(df)\n",
    "model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "print(model.summary())\n",
    "# plot residuals vs fitted values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(model.fittedvalues, model.resid)\n",
    "ax.set_xlabel('Fitted Values')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residuals vs Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model above, can predict the prices of houses based on certain features, such as the number of bedrooms, the square footage, the year the house was built, and others.\n",
    "\n",
    "The first part of the code prepares the data for the model by standardizing the numeric features, meaning that it scales them so that they are all on the same scale and centered around 0. This helps the model to be more accurate.\n",
    "\n",
    "The second part of the code fits the linear regression model to the data and produces a summary of the results. This summary shows the R-squared value, which is a measure of how well the model fits the data (the closer to 1, the better), as well as the coefficients for each feature, which indicate how strongly each feature affects the price.\n",
    "\n",
    "The third part of the code creates a scatter plot that shows the relationship between the predicted prices and the actual prices of the houses in the dataset. This helps to visualize how well the model is predicting the prices, and to identify any outliers or areas where the model is particularly inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression analysis is using the Ordinary Least Squares (OLS) method, which is a statistical technique for predicting the relationship between a dependent variable (y) and one or more independent variables (x1, x2, x3, ... xn).\n",
    "\n",
    "In this particular case, there are 30 independent variables (x1 to x30), and the results show that the model has a very good fit, with an R-squared value of 1.000. The R-squared value is a measure of how well the model explains the variation in the dependent variable. In this case, the value of 1.000 means that the model explains 100% of the variation in the dependent variable.\n",
    "\n",
    "The table shows the estimated coefficients for each independent variable, along with their standard errors, t-values, and p-values. The coefficients represent the slope of the line that relates each independent variable to the dependent variable. The t-values and p-values are used to test whether the coefficients are significantly different from zero. A p-value less than 0.05 is typically used to indicate statistical significance.\n",
    "\n",
    "In summary, this analysis suggests that the independent variables are highly predictive of the dependent variable, and that the model fits the data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** pointers\n",
    ">attribute excelent model performance to good data understanding\n",
    ">as well as keen data clean up\n",
    ">also consider knowledge derived from initial models. \n",
    ">recomdend accept our findings as valid support of our busines questions but specify we are not 100% confident it is an actual representation of larger population/country\n",
    ">suggest areas of investment considering what we observed from our visualizations \n",
    ">write conslusion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
